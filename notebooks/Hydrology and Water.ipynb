{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77432fa7-bc2c-4788-8fab-dad948ff61d4",
   "metadata": {},
   "source": [
    "# Section II\n",
    "Hydrology\n",
    "Validate your installations, in a notebook by executing:\n",
    "\n",
    "!pip install earthengine-api geemap folium matplotlib pandas geopandas rasterio rasterstats scipy scikit-learn seaborn statsmodels !earthengine authenticate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c0dd1f-3d2f-4aeb-8f8d-bbd47fbeeea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace by your project numbber:\n",
    "my_project = 'my-project-0000000000000000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41e91ab-81b4-4297-9ccc-3923d231092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth Engine initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "from datetime import datetime, timezone\n",
    "# Initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize(project=my_project)\n",
    "    print(\"Earth Engine initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Earth Engine: {e}\")\n",
    "    print(\"Please authenticate using: ee.Authenticate()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21973f5c-d22a-448a-9641-be5f2c759bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_surface_water_mapping(region, date_range, confidence_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Creates comprehensive surface water maps using multi-sensor fusion.\n",
    "    Combines Landsat, Sentinel-1 SAR, and auxiliary datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_date, end_date = date_range\n",
    "    \n",
    "    # 1. Optical water detection using multiple indices\n",
    "    def optical_water_detection(image):\n",
    "        \"\"\"Enhanced optical water detection using multiple spectral indices.\"\"\"\n",
    "        \n",
    "        # Calculate water indices\n",
    "        green = image.select('SR_B3').multiply(0.0000275).add(-0.2)\n",
    "        nir = image.select('SR_B5').multiply(0.0000275).add(-0.2)\n",
    "        swir1 = image.select('SR_B6').multiply(0.0000275).add(-0.2)\n",
    "        swir2 = image.select('SR_B7').multiply(0.0000275).add(-0.2)\n",
    "        \n",
    "        # Multiple water indices for robust detection\n",
    "        ndwi = green.subtract(nir).divide(green.add(nir)).rename('NDWI')\n",
    "        mndwi = green.subtract(swir1).divide(green.add(swir1)).rename('MNDWI')\n",
    "        awesh = green.add(swir1.multiply(2.5)).subtract(nir.multiply(1.5)).subtract(swir2.multiply(0.25)).rename('AWESH')\n",
    "        \n",
    "        # Combine indices using weighted approach\n",
    "        water_probability = ndwi.multiply(0.4) \\\n",
    "            .add(mndwi.multiply(0.4)) \\\n",
    "            .add(awesh.multiply(0.2)) \\\n",
    "            .rename('water_probability_optical')\n",
    "        \n",
    "        # Apply dynamic thresholds based on local conditions\n",
    "        water_mask_optical = water_probability.gt(0.3)\n",
    "        \n",
    "        return image.addBands([water_probability, water_mask_optical])\n",
    "    \n",
    "    # Process Landsat collection\n",
    "    landsat_collection = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2') \\\n",
    "        .filterBounds(region) \\\n",
    "        .filterDate(start_date, end_date) \\\n",
    "        .map(advanced_cloud_mask) \\\n",
    "        .map(optical_water_detection)\n",
    "    \n",
    "    optical_water_composite = landsat_collection.median()\n",
    "    \n",
    "    # 2. SAR-based water detection\n",
    "    def sar_water_detection(image):\n",
    "        \"\"\"Water detection using Sentinel-1 SAR data.\"\"\"\n",
    "        \n",
    "        # VV polarization is most sensitive to water\n",
    "        vv = image.select('VV')\n",
    "        vh = image.select('VH')\n",
    "        \n",
    "        # Apply speckle filtering\n",
    "        vv_filtered = vv.focalMean(1, 'square', 'pixels')\n",
    "        vh_filtered = vh.focalMean(1, 'square', 'pixels')\n",
    "        \n",
    "        # Water typically has very low backscatter in VV\n",
    "        water_mask_vv = vv_filtered.lt(-18)  # Threshold in dB\n",
    "        \n",
    "        # Cross-polarization ratio for additional discrimination\n",
    "        vh_vv_ratio = vh_filtered.subtract(vv_filtered)\n",
    "        water_mask_ratio = vh_vv_ratio.lt(-8)\n",
    "        \n",
    "        # Combine SAR indicators\n",
    "        water_probability_sar = ee.Image(0) \\\n",
    "            .where(water_mask_vv.And(water_mask_ratio), 1) \\\n",
    "            .where(water_mask_vv.Or(water_mask_ratio), 0.5) \\\n",
    "            .rename('water_probability_sar')\n",
    "        \n",
    "        return image.addBands(water_probability_sar)\n",
    "    \n",
    "    # Process Sentinel-1 collection\n",
    "    sentinel1_collection = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "        .filterBounds(region) \\\n",
    "        .filterDate(start_date, end_date) \\\n",
    "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
    "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "        .map(sar_water_detection)\n",
    "    \n",
    "    sar_water_composite = sentinel1_collection.median()\n",
    "    \n",
    "    # 3. Multi-sensor fusion\n",
    "    def fuse_water_detections(optical_prob, sar_prob, dem, slope):\n",
    "        \"\"\"\n",
    "        Fuses optical and SAR water detections using topographic context.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weight SAR more heavily in areas prone to optical limitations\n",
    "        # (e.g., areas with frequent clouds, shadows)\n",
    "        optical_weight = ee.Image(0.6)  # Base weight for optical\n",
    "        sar_weight = ee.Image(0.4)      # Base weight for SAR\n",
    "        \n",
    "        # Adjust weights based on topographic context\n",
    "        # SAR performs better in flat areas, optical in steep terrain\n",
    "        flat_areas = slope.lt(5)  # Degrees\n",
    "        optical_weight = optical_weight.where(flat_areas, 0.4)\n",
    "        sar_weight = sar_weight.where(flat_areas, 0.6)\n",
    "        \n",
    "        # Combine probabilities\n",
    "        fused_probability = optical_prob.multiply(optical_weight) \\\n",
    "            .add(sar_prob.multiply(sar_weight))\n",
    "        \n",
    "        # Apply topographic constraints\n",
    "        # Water unlikely above certain elevation or on steep slopes\n",
    "        elevation_mask = dem.lt(3000)  # Meters above sea level\n",
    "        slope_mask = slope.lt(15)      # Degrees\n",
    "        topographic_constraint = elevation_mask.And(slope_mask)\n",
    "        \n",
    "        final_probability = fused_probability.updateMask(topographic_constraint)\n",
    "        final_water_mask = final_probability.gt(confidence_threshold)\n",
    "        \n",
    "        return {\n",
    "            'water_probability': final_probability,\n",
    "            'water_mask': final_water_mask,\n",
    "            'fusion_weights': ee.Image.cat([optical_weight, sar_weight]).rename(['optical_weight', 'sar_weight'])\n",
    "        }\n",
    "    \n",
    "    # Get topographic data\n",
    "    dem = ee.Image('USGS/SRTMGL1_003')\n",
    "    slope = ee.Terrain.slope(dem)\n",
    "    \n",
    "    # Perform fusion\n",
    "    fusion_result = fuse_water_detections(\n",
    "        optical_water_composite.select('water_probability_optical'),\n",
    "        sar_water_composite.select('water_probability_sar'),\n",
    "        dem,\n",
    "        slope\n",
    "    )\n",
    "    \n",
    "    return fusion_result\n",
    "\n",
    "# 4. Temporal water dynamics analysis\n",
    "def analyze_water_dynamics(water_time_series, region):\n",
    "    \"\"\"\n",
    "    Analyzes temporal patterns in surface water extent.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate water occurrence frequency\n",
    "    water_occurrence = water_time_series.select('water_mask').mean().rename('water_occurrence')\n",
    "    \n",
    "    # Classify water persistence\n",
    "    permanent_water = water_occurrence.gt(0.8)\n",
    "    seasonal_water = water_occurrence.gt(0.2).And(water_occurrence.lte(0.8))\n",
    "    ephemeral_water = water_occurrence.gt(0.05).And(water_occurrence.lte(0.2))\n",
    "    \n",
    "    # Calculate seasonal patterns\n",
    "    def seasonal_analysis(collection):\n",
    "        seasons = {\n",
    "            'spring': [3, 4, 5],\n",
    "            'summer': [6, 7, 8], \n",
    "            'autumn': [9, 10, 11],\n",
    "            'winter': [12, 1, 2]\n",
    "        }\n",
    "        \n",
    "        seasonal_water = {}\n",
    "        for season, months in seasons.items():\n",
    "            seasonal_collection = collection.filter(\n",
    "                ee.Filter.calendarRange(months[0], months[-1], 'month')\n",
    "            )\n",
    "            seasonal_water[season] = seasonal_collection.select('water_mask').mean()\n",
    "        \n",
    "        return seasonal_water\n",
    "    \n",
    "    seasonal_patterns = seasonal_analysis(water_time_series)\n",
    "    \n",
    "    # Calculate water extent time series\n",
    "    def calculate_water_extent(image):\n",
    "        extent = image.select('water_mask').reduceRegion(\n",
    "            reducer=ee.Reducer.sum(),\n",
    "            geometry=region,\n",
    "            scale=30,\n",
    "            maxPixels=1e9\n",
    "        ).get('water_mask')\n",
    "        \n",
    "        return ee.Feature(None, {\n",
    "            'date': image.date().format('YYYY-MM-dd'),\n",
    "            'water_extent_pixels': extent,\n",
    "            'water_extent_hectares': ee.Number(extent).multiply(0.09)  # 30m pixel = 0.09 ha\n",
    "        })\n",
    "    \n",
    "    extent_time_series = water_time_series.map(calculate_water_extent)\n",
    "    \n",
    "    return {\n",
    "        'water_occurrence': water_occurrence,\n",
    "        'permanent_water': permanent_water,\n",
    "        'seasonal_water': seasonal_water,\n",
    "        'ephemeral_water': ephemeral_water,\n",
    "        'seasonal_patterns': seasonal_patterns,\n",
    "        'extent_time_series': extent_time_series\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafa599d-d84e-415d-89a2-a352ba749d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_precipitation_analysis(region, analysis_period, reference_stations=None):\n",
    "    \"\"\"\n",
    "    Comprehensive precipitation analysis combining satellite data with ground observations.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_date, end_date = analysis_period\n",
    "    \n",
    "    # 1. Multi-source precipitation data integration\n",
    "    def integrate_precipitation_sources():\n",
    "        \"\"\"Integrates multiple precipitation datasets for robust analysis.\"\"\"\n",
    "        \n",
    "        # CHIRPS - Climate Hazards Group InfraRed Precipitation with Station data\n",
    "        chirps = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY') \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .filterBounds(region)\n",
    "        \n",
    "        # GPM IMERG - Global Precipitation Measurement Integrated Multi-satellitE Retrievals\n",
    "        gpm = ee.ImageCollection('NASA/GPM_L3/IMERG_V06') \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .filterBounds(region) \\\n",
    "            .select('precipitationCal')\n",
    "        \n",
    "        # TRMM (if analyzing historical period)\n",
    "        trmm = ee.ImageCollection('TRMM/3B42') \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .filterBounds(region) \\\n",
    "            .select('precipitation')\n",
    "        \n",
    "        # Bias correction and harmonization\n",
    "        def harmonize_precipitation_data(chirps_col, gpm_col):\n",
    "            \"\"\"Harmonizes different precipitation products.\"\"\"\n",
    "            \n",
    "            # Resample GPM to CHIRPS resolution (0.05°)\n",
    "            def resample_gpm(image):\n",
    "                return image.resample('bilinear').reproject(\n",
    "                    crs=chirps_col.first().projection(),\n",
    "                    scale=5566  # CHIRPS native resolution\n",
    "                )\n",
    "            \n",
    "            gpm_resampled = gpm_col.map(resample_gpm)\n",
    "            \n",
    "            # Calculate bias correction factors\n",
    "            def calculate_bias_correction():\n",
    "                chirps_mean = chirps_col.mean()\n",
    "                gpm_mean = gpm_resampled.mean()\n",
    "                bias_factor = chirps_mean.divide(gpm_mean)\n",
    "                return bias_factor\n",
    "            \n",
    "            bias_correction = calculate_bias_correction()\n",
    "            \n",
    "            # Apply bias correction to GPM\n",
    "            def apply_bias_correction(image):\n",
    "                return image.multiply(bias_correction).copyProperties(image, ['system:time_start'])\n",
    "            \n",
    "            gpm_corrected = gpm_resampled.map(apply_bias_correction)\n",
    "            \n",
    "            return chirps_col, gpm_corrected\n",
    "        \n",
    "        chirps_harmonized, gpm_harmonized = harmonize_precipitation_data(chirps, gpm)\n",
    "        \n",
    "        return {\n",
    "            'chirps': chirps_harmonized,\n",
    "            'gpm': gpm_harmonized,\n",
    "            'trmm': trmm\n",
    "        }\n",
    "    \n",
    "    precipitation_sources = integrate_precipitation_sources()\n",
    "    \n",
    "    # 2. Extreme precipitation event detection\n",
    "    def detect_extreme_precipitation_events(precip_collection, return_periods=[2, 5, 10, 25]):\n",
    "        \"\"\"\n",
    "        Detects extreme precipitation events using statistical analysis.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate daily precipitation statistics\n",
    "        daily_stats = precip_collection.map(lambda img: \n",
    "            img.reduceRegion(\n",
    "                reducer=ee.Reducer.mean().combine(ee.Reducer.max(), sharedInputs=True),\n",
    "                geometry=region,\n",
    "                scale=5566,\n",
    "                maxPixels=1e8\n",
    "            ).set('date', img.date().format('YYYY-MM-dd'))\n",
    "        )\n",
    "        \n",
    "        # Calculate precipitation percentiles for extreme event thresholds\n",
    "        precipitation_percentiles = precip_collection.reduce(ee.Reducer.percentile([90, 95, 99, 99.9]))\n",
    "        \n",
    "        # Identify extreme events\n",
    "        def classify_extreme_events(image):\n",
    "            p90 = precipitation_percentiles.select('precipitation_p90')\n",
    "            p95 = precipitation_percentiles.select('precipitation_p95')\n",
    "            p99 = precipitation_percentiles.select('precipitation_p99')\n",
    "            \n",
    "            moderate_extreme = image.gt(p90).And(image.lte(p95))\n",
    "            severe_extreme = image.gt(p95).And(image.lte(p99))\n",
    "            extreme_extreme = image.gt(p99)\n",
    "            \n",
    "            return image.addBands([\n",
    "                moderate_extreme.rename('moderate_extreme'),\n",
    "                severe_extreme.rename('severe_extreme'),\n",
    "                extreme_extreme.rename('extreme_extreme')\n",
    "            ])\n",
    "        \n",
    "        extreme_events = precip_collection.map(classify_extreme_events)\n",
    "        \n",
    "        return {\n",
    "            'extreme_events': extreme_events,\n",
    "            'percentile_thresholds': precipitation_percentiles,\n",
    "            'daily_stats': daily_stats\n",
    "        }\n",
    "    \n",
    "    extreme_analysis = detect_extreme_precipitation_events(precipitation_sources['chirps'])\n",
    "    \n",
    "    # 3. Drought monitoring using standardized precipitation index (SPI)\n",
    "    def calculate_standardized_precipitation_index(precip_collection, time_scales=[1, 3, 6, 12]):\n",
    "        \"\"\"\n",
    "        Calculates SPI for multiple time scales for drought monitoring.\n",
    "        \"\"\"\n",
    "        \n",
    "        spi_results = {}\n",
    "        \n",
    "        for time_scale in time_scales:\n",
    "            # Calculate rolling sum for the time scale (in months)\n",
    "            def calculate_rolling_precipitation(image):\n",
    "                # Get date of current image\n",
    "                current_date = image.date()\n",
    "                \n",
    "                # Calculate start date for rolling window\n",
    "                start_date = current_date.advance(-time_scale, 'month')\n",
    "                \n",
    "                # Filter collection for rolling window\n",
    "                rolling_collection = precip_collection.filterDate(start_date, current_date)\n",
    "                \n",
    "                # Sum precipitation over window\n",
    "                rolling_sum = rolling_collection.sum()\n",
    "                \n",
    "                return rolling_sum.set('date', current_date.format('YYYY-MM-dd'))\n",
    "            \n",
    "            # Calculate rolling precipitation\n",
    "            rolling_precip = precip_collection.map(calculate_rolling_precipitation)\n",
    "            \n",
    "            # Calculate SPI (simplified - assumes gamma distribution)\n",
    "            precip_mean = rolling_precip.mean()\n",
    "            precip_std = rolling_precip.reduce(ee.Reducer.stdDev())\n",
    "            \n",
    "            def calculate_spi(image):\n",
    "                # Standardize precipitation values\n",
    "                spi = image.subtract(precip_mean).divide(precip_std)\n",
    "                return spi.rename(f'SPI_{time_scale}month')\n",
    "            \n",
    "            spi_collection = rolling_precip.map(calculate_spi)\n",
    "            spi_results[f'{time_scale}_month'] = spi_collection\n",
    "        \n",
    "        return spi_results\n",
    "    \n",
    "    spi_analysis = calculate_standardized_precipitation_index(precipitation_sources['chirps'])\n",
    "    \n",
    "    # 4. Hydrological modeling integration\n",
    "    def integrate_hydrological_modeling(precipitation_data, catchment_geometry):\n",
    "        \"\"\"\n",
    "        Integrates precipitation data with simple hydrological modeling.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get additional data for hydrological modeling\n",
    "        dem = ee.Image('USGS/SRTMGL1_003')\n",
    "        slope = ee.Terrain.slope(dem)\n",
    "        \n",
    "        # Land cover for runoff coefficient estimation\n",
    "        landcover = ee.Image('ESA/WorldCover/v100/2020')\n",
    "        \n",
    "        # Soil data (example using global datasets)\n",
    "        soil_properties = ee.Image('OpenLandMap/SOL/SOL_WATERCONTENT-33KPA_USDA-4A1H_M/v01') \\\n",
    "            .select('b0')  # Water content at field capacity\n",
    "        \n",
    "        # Simple runoff calculation using SCS Curve Number method (simplified)\n",
    "        def calculate_runoff_coefficient(landcover_img, soil_img, slope_img):\n",
    "            \"\"\"Calculate runoff coefficient based on land cover and soil properties.\"\"\"\n",
    "            \n",
    "            # Simplified curve numbers for different land covers\n",
    "            curve_numbers = ee.Image(0) \\\n",
    "                .where(landcover_img.eq(10), 30) \\\n",
    "                .where(landcover_img.eq(20), 55) \\\n",
    "                .where(landcover_img.eq(40), 70) \\\n",
    "                .where(landcover_img.eq(50), 85) \\\n",
    "                .where(landcover_img.eq(80), 95) \\\n",
    "                .rename('curve_number')\n",
    "            \n",
    "            # Adjust for slope (higher slopes = higher runoff)\n",
    "            slope_factor = slope_img.divide(45).multiply(0.2).add(0.8)  # Range 0.8-1.0\n",
    "            \n",
    "            adjusted_cn = curve_numbers.multiply(slope_factor)\n",
    "            runoff_coefficient = adjusted_cn.divide(100)\n",
    "            \n",
    "            return runoff_coefficient\n",
    "        \n",
    "        runoff_coeff = calculate_runoff_coefficient(landcover, soil_properties, slope)\n",
    "        \n",
    "        # Calculate potential runoff\n",
    "        def calculate_runoff(precip_image):\n",
    "            potential_runoff = precip_image.multiply(runoff_coeff)\n",
    "            \n",
    "            # Apply initial abstraction (simplified)\n",
    "            initial_abstraction = runoff_coeff.multiply(-5)  # mm\n",
    "            actual_runoff = potential_runoff.add(initial_abstraction).max(0)\n",
    "            \n",
    "            return precip_image.addBands([\n",
    "                potential_runoff.rename('potential_runoff'),\n",
    "                actual_runoff.rename('actual_runoff')\n",
    "            ])\n",
    "        \n",
    "        runoff_collection = precipitation_sources['chirps'].map(calculate_runoff)\n",
    "        \n",
    "        # Calculate catchment-scale runoff\n",
    "        def calculate_catchment_runoff(image):\n",
    "            catchment_runoff = image.select('actual_runoff').reduceRegion(\n",
    "                reducer=ee.Reducer.mean(),\n",
    "                geometry=catchment_geometry,\n",
    "                scale=5566,\n",
    "                maxPixels=1e8\n",
    "            )\n",
    "            \n",
    "            return ee.Feature(None, {\n",
    "                'date': image.date().format('YYYY-MM-dd'),\n",
    "                'mean_runoff_mm': catchment_runoff.get('actual_runoff')\n",
    "            })\n",
    "        \n",
    "        catchment_runoff_series = runoff_collection.map(calculate_catchment_runoff)\n",
    "        \n",
    "        return {\n",
    "            'runoff_collection': runoff_collection,\n",
    "            'runoff_coefficient': runoff_coeff,\n",
    "            'catchment_runoff_series': catchment_runoff_series\n",
    "        }\n",
    "    \n",
    "    # Apply hydrological modeling if catchment geometry is provided\n",
    "    if region.type().getInfo() == 'Polygon':\n",
    "        hydrological_analysis = integrate_hydrological_modeling(\n",
    "            precipitation_sources['chirps'], \n",
    "            region\n",
    "        )\n",
    "    else:\n",
    "        hydrological_analysis = None\n",
    "    \n",
    "    return {\n",
    "        'precipitation_sources': precipitation_sources,\n",
    "        'extreme_analysis': extreme_analysis,\n",
    "        'spi_analysis': spi_analysis,\n",
    "        'hydrological_analysis': hydrological_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6976c2b7-a15b-48ac-8cbd-182b1a39277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_climate_analysis(region, analysis_period, climate_variables=None):\n",
    "    \"\"\"\n",
    "    Comprehensive climate analysis using multiple reanalysis products.\n",
    "    \"\"\"\n",
    "    \n",
    "    if climate_variables is None:\n",
    "        climate_variables = [\n",
    "            'temperature', 'precipitation', 'humidity', 'wind', \n",
    "            'pressure', 'radiation', 'evapotranspiration'\n",
    "        ]\n",
    "    \n",
    "    start_date, end_date = analysis_period\n",
    "    \n",
    "    # Define climate data sources\n",
    "    climate_datasets = {\n",
    "        'era5_monthly': 'ECMWF/ERA5/MONTHLY',\n",
    "        'era5_daily': 'ECMWF/ERA5_LAND/DAILY_AGGR',\n",
    "        'terraclimate': 'IDAHO_EPSCOR/TERRACLIMATE',\n",
    "        'gridmet': 'IDAHO_EPSCOR/GRIDMET',\n",
    "        'chirps': 'UCSB-CHG/CHIRPS/DAILY'\n",
    "    }\n",
    "    \n",
    "    def extract_comprehensive_climate_data():\n",
    "        \"\"\"Extract and harmonize climate variables from multiple sources.\"\"\"\n",
    "        \n",
    "        climate_data = {}\n",
    "        \n",
    "        # ERA5 Monthly - High-quality reanalysis for long-term trends\n",
    "        if 'temperature' in climate_variables or 'humidity' in climate_variables:\n",
    "            era5_monthly = ee.ImageCollection('ECMWF/ERA5/MONTHLY') \\\n",
    "                .filterDate(start_date, end_date) \\\n",
    "                .filterBounds(region)\n",
    "            \n",
    "            climate_data['era5_monthly'] = {\n",
    "                'collection': era5_monthly,\n",
    "                'variables': {\n",
    "                    'temperature_2m': 'mean_2m_air_temperature',\n",
    "                    'temperature_max': 'maximum_2m_air_temperature_since_previous_post_processing',\n",
    "                    'temperature_min': 'minimum_2m_air_temperature_since_previous_post_processing',\n",
    "                    'dewpoint': 'dewpoint_2m_temperature',\n",
    "                    'humidity': 'relative_humidity_2m',\n",
    "                    'pressure': 'surface_pressure',\n",
    "                    'wind_u': '10m_u_component_of_wind',\n",
    "                    'wind_v': '10m_v_component_of_wind'\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # TerraClimate - High resolution climate data\n",
    "        terraclimate = ee.ImageCollection('IDAHO_EPSCOR/TERRACLIMATE') \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .filterBounds(region)\n",
    "        \n",
    "        climate_data['terraclimate'] = {\n",
    "            'collection': terraclimate,\n",
    "            'variables': {\n",
    "                'temperature_max': 'tmmx',\n",
    "                'temperature_min': 'tmmn',\n",
    "                'precipitation': 'pr',\n",
    "                'vapor_pressure_deficit': 'vpd',\n",
    "                'wind_speed': 'vs',\n",
    "                'soil_moisture': 'soil',\n",
    "                'evapotranspiration': 'aet',\n",
    "                'potential_evapotranspiration': 'pet'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # GridMET - High resolution meteorological data for continental US\n",
    "        if region.bounds().getInfo()['coordinates'][0][0][0] > -180:  # Check if in Americas\n",
    "            gridmet = ee.ImageCollection('IDAHO_EPSCOR/GRIDMET') \\\n",
    "                .filterDate(start_date, end_date) \\\n",
    "                .filterBounds(region)\n",
    "            \n",
    "            climate_data['gridmet'] = {\n",
    "                'collection': gridmet,\n",
    "                'variables': {\n",
    "                    'temperature_max': 'tmmx',\n",
    "                    'temperature_min': 'tmmn',\n",
    "                    'precipitation': 'pr',\n",
    "                    'humidity': 'rmax',\n",
    "                    'wind_speed': 'vs',\n",
    "                    'solar_radiation': 'srad',\n",
    "                    'vapor_pressure_deficit': 'vpd',\n",
    "                    'evapotranspiration': 'etr'\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return climate_data\n",
    "    \n",
    "    climate_data_sources = extract_comprehensive_climate_data()\n",
    "    \n",
    "    # Climate trend analysis\n",
    "    def analyze_climate_trends(data_sources):\n",
    "        \"\"\"\n",
    "        Analyzes long-term climate trends using multiple statistical methods.\n",
    "        \"\"\"\n",
    "        \n",
    "        trend_results = {}\n",
    "        \n",
    "        for source_name, source_data in data_sources.items():\n",
    "            collection = source_data['collection']\n",
    "            variables = source_data['variables']\n",
    "            \n",
    "            source_trends = {}\n",
    "            \n",
    "            for var_name, band_name in variables.items():\n",
    "                if var_name not in climate_variables:\n",
    "                    continue\n",
    "                \n",
    "                # Extract time series for the variable\n",
    "                def extract_regional_mean(image):\n",
    "                    # Handle unit conversions for different datasets\n",
    "                    band = image.select(band_name)\n",
    "                    \n",
    "                    # Convert temperature from Kelvin to Celsius if needed\n",
    "                    if 'temperature' in var_name and source_name == 'era5_monthly':\n",
    "                        band = band.subtract(273.15)\n",
    "                    \n",
    "                    # Convert precipitation to mm/day if needed\n",
    "                    if var_name == 'precipitation' and source_name == 'terraclimate':\n",
    "                        band = band.multiply(0.1)  # TerraClimate precipitation is in mm/month\n",
    "\n",
    "                    scaling = 27830 if source_name == 'era5_monthly' else 4638\n",
    "                    \n",
    "                    regional_mean = band.reduceRegion(\n",
    "                        reducer=ee.Reducer.mean(),\n",
    "                        geometry=region,\n",
    "                        scale=scaling,  # Different scales for different datasets\n",
    "                        maxPixels=1e12\n",
    "                    ).get(band_name)\n",
    "                    \n",
    "                    return ee.Feature(None, {\n",
    "                        'date': image.date().format('YYYY-MM-dd'),\n",
    "                        'timestamp': image.get('system:time_start'),\n",
    "                        'value': regional_mean,\n",
    "                        'variable': var_name,\n",
    "                        'source': source_name\n",
    "                    })\n",
    "                \n",
    "                time_series = collection.map(extract_regional_mean)\n",
    "                \n",
    "                # Calculate trend statistics using linear regression\n",
    "                def calculate_trend_statistics(time_series_fc):\n",
    "                    \"\"\"Calculate linear trend and statistical significance.\"\"\"\n",
    "                    \n",
    "                    # This would typically be done client-side with more sophisticated statistics\n",
    "                    # Here we show the server-side approach for trend detection\n",
    "                    \n",
    "                    # Convert to array for linear regression\n",
    "                    time_series_array = time_series_fc.map(lambda f: \n",
    "                        ee.Feature(None, {\n",
    "                            'x': ee.Number(f.get('timestamp')).divide(1000*60*60*24*365.25),  # Years since epoch\n",
    "                            'y': f.get('value')\n",
    "                        })\n",
    "                    )\n",
    "                    \n",
    "                    # Simple linear regression (simplified)\n",
    "                    x_values = time_series_array.aggregate_array('x')\n",
    "                    y_values = time_series_array.aggregate_array('y')\n",
    "                    \n",
    "                    # Calculate correlation coefficient\n",
    "                    correlation = ee.Array(x_values).subtract(ee.Array(x_values).reduce(ee.Reducer.mean(), [0])) \\\n",
    "                        .multiply(ee.Array(y_values).subtract(ee.Array(y_values).reduce(ee.Reducer.mean(), [0]))) \\\n",
    "                        .reduce(ee.Reducer.sum(), [0])\n",
    "                    \n",
    "                    return {\n",
    "                        'time_series': time_series_array,\n",
    "                        'trend_direction': correlation.gt(0)\n",
    "                    }\n",
    "                \n",
    "                trend_stats = calculate_trend_statistics(time_series)\n",
    "                source_trends[var_name] = {\n",
    "                    'time_series': time_series,\n",
    "                    'trend_statistics': trend_stats\n",
    "                }\n",
    "            \n",
    "            trend_results[source_name] = source_trends\n",
    "        \n",
    "        return trend_results\n",
    "    \n",
    "    trend_analysis = analyze_climate_trends(climate_data_sources)\n",
    "    \n",
    "    # Climate extreme analysis\n",
    "    def analyze_climate_extremes(data_sources):\n",
    "        \"\"\"\n",
    "        Analyzes climate extremes including heat waves, cold spells, and extreme precipitation.\n",
    "        \"\"\"\n",
    "        \n",
    "        extreme_results = {}\n",
    "        \n",
    "        for source_name, source_data in data_sources.items():\n",
    "            collection = source_data['collection']\n",
    "            variables = source_data['variables']\n",
    "            \n",
    "            # Temperature extremes\n",
    "            if 'temperature_max' in variables and 'temperature_min' in variables:\n",
    "                temp_max_band = variables['temperature_max']\n",
    "                temp_min_band = variables['temperature_min']\n",
    "                \n",
    "                # Calculate temperature thresholds (90th and 10th percentiles)\n",
    "                temp_max_p90 = collection.select(temp_max_band).reduce(ee.Reducer.percentile([90]))\n",
    "                temp_min_p10 = collection.select(temp_min_band).reduce(ee.Reducer.percentile([10]))\n",
    "                \n",
    "                # Identify extreme temperature events\n",
    "                def identify_temperature_extremes(image):\n",
    "                    temp_max = image.select(temp_max_band)\n",
    "                    temp_min = image.select(temp_min_band)\n",
    "                    \n",
    "                    heat_wave_day = temp_max.gt(temp_max_p90)\n",
    "                    cold_spell_day = temp_min.lt(temp_min_p10)\n",
    "                    \n",
    "                    return image.addBands([\n",
    "                        heat_wave_day.rename('heat_wave_day'),\n",
    "                        cold_spell_day.rename('cold_spell_day')\n",
    "                    ])\n",
    "                \n",
    "                extreme_temp_collection = collection.map(identify_temperature_extremes)\n",
    "                \n",
    "                extreme_results[f'{source_name}_temperature_extremes'] = {\n",
    "                    'collection': extreme_temp_collection,\n",
    "                    'thresholds': {\n",
    "                        'heat_wave_threshold': temp_max_p90,\n",
    "                        'cold_spell_threshold': temp_min_p10\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # Precipitation extremes\n",
    "            if 'precipitation' in variables:\n",
    "                precip_band = variables['precipitation']\n",
    "                \n",
    "                # Calculate extreme precipitation thresholds\n",
    "                precip_thresholds = collection.select(precip_band).reduce(\n",
    "                    ee.Reducer.percentile([95, 99, 99.9])\n",
    "                )\n",
    "                \n",
    "                def identify_precipitation_extremes(image):\n",
    "                    precip = image.select(precip_band)\n",
    "                    \n",
    "                    extreme_precip_p95 = precip.gt(precip_thresholds.select(f'{precip_band}_p95'))\n",
    "                    extreme_precip_p99 = precip.gt(precip_thresholds.select(f'{precip_band}_p99'))\n",
    "                    extreme_precip_p999 = precip.gt(precip_thresholds.select(f'{precip_band}_p99_9'))\n",
    "                    \n",
    "                    return image.addBands([\n",
    "                        extreme_precip_p95.rename('extreme_precip_p95'),\n",
    "                        extreme_precip_p99.rename('extreme_precip_p99'),\n",
    "                        extreme_precip_p999.rename('extreme_precip_p999')\n",
    "                    ])\n",
    "                \n",
    "                extreme_precip_collection = collection.map(identify_precipitation_extremes)\n",
    "                \n",
    "                extreme_results[f'{source_name}_precipitation_extremes'] = {\n",
    "                    'collection': extreme_precip_collection,\n",
    "                    'thresholds': precip_thresholds\n",
    "                }\n",
    "        \n",
    "        return extreme_results\n",
    "    \n",
    "    extreme_analysis = analyze_climate_extremes(climate_data_sources)\n",
    "    \n",
    "    # Climate change impact assessment\n",
    "    def assess_climate_change_impacts(trend_data, extreme_data, baseline_period=None):\n",
    "        \"\"\"\n",
    "        Assesses climate change impacts using trend and extreme event analysis.\n",
    "        \"\"\"\n",
    "        \n",
    "        if baseline_period is None:\n",
    "            baseline_period = ('1981-01-01', '2010-12-31')\n",
    "        \n",
    "        impact_assessment = {}\n",
    "        \n",
    "        # Temperature change assessment\n",
    "        temperature_trends = []\n",
    "        for source in trend_data:\n",
    "            for variable in trend_data[source]:\n",
    "                if 'temperature' in variable:\n",
    "                    temperature_trends.append(trend_data[source][variable])\n",
    "        \n",
    "        # Precipitation change assessment\n",
    "        precipitation_trends = []\n",
    "        for source in trend_data:\n",
    "            for variable in trend_data[source]:\n",
    "                if 'precipitation' in variable:\n",
    "                    precipitation_trends.append(trend_data[source][variable])\n",
    "        \n",
    "        # Extreme event frequency changes\n",
    "        extreme_frequency_changes = {}\n",
    "        for extreme_type in extreme_data:\n",
    "            collection = extreme_data[extreme_type]['collection']\n",
    "            \n",
    "            # Calculate frequency during baseline vs. recent period\n",
    "            baseline_collection = collection.filterDate(baseline_period[0], baseline_period[1])\n",
    "            recent_collection = collection.filterDate('2010-01-01', end_date)\n",
    "            \n",
    "            # This would involve more detailed statistical analysis in practice\n",
    "            extreme_frequency_changes[extreme_type] = {\n",
    "                'baseline_collection': baseline_collection,\n",
    "                'recent_collection': recent_collection\n",
    "            }\n",
    "        \n",
    "        impact_assessment = {\n",
    "            'temperature_trends': temperature_trends,\n",
    "            'precipitation_trends': precipitation_trends,\n",
    "            'extreme_frequency_changes': extreme_frequency_changes\n",
    "        }\n",
    "        \n",
    "        return impact_assessment\n",
    "    \n",
    "    impact_assessment = assess_climate_change_impacts(trend_analysis, extreme_analysis)\n",
    "    \n",
    "    return {\n",
    "        'climate_data_sources': climate_data_sources,\n",
    "        'trend_analysis': trend_analysis,\n",
    "        'extreme_analysis': extreme_analysis,\n",
    "        'impact_assessment': impact_assessment\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137cb1c8-e9f2-42b1-b538-a26e9e7ac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extreme_event_monitoring_system(region, monitoring_variables=None):\n",
    "    \"\"\"\n",
    "    Creates an integrated system for monitoring multiple types of extreme events.\n",
    "    \"\"\"\n",
    "    \n",
    "    if monitoring_variables is None:\n",
    "        monitoring_variables = ['flood', 'drought', 'heat_wave', 'extreme_precipitation']\n",
    "    \n",
    "    monitoring_system = {}\n",
    "    \n",
    "    # 1. Multi-hazard drought monitoring\n",
    "    def setup_drought_monitoring():\n",
    "        \"\"\"\n",
    "        Sets up comprehensive drought monitoring using multiple indicators.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Palmer Drought Severity Index (PDSI) from TerraClimate\n",
    "        terraclimate = ee.ImageCollection('IDAHO_EPSCOR/TERRACLIMATE')\n",
    "        pdsi = terraclimate.select('pdsi')\n",
    "        \n",
    "        # Standardized Precipitation Index (SPI) - calculated from CHIRPS\n",
    "        chirps = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')\n",
    "        \n",
    "        # Vegetation health from MODIS\n",
    "        modis_ndvi = ee.ImageCollection('MODIS/006/MOD13A1').select('NDVI')\n",
    "        \n",
    "        # Soil moisture from ERA5\n",
    "        era5_land = ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR')\n",
    "        soil_moisture = era5_land.select('volumetric_soil_water_layer_1')\n",
    "        \n",
    "        def calculate_drought_composite_index(date):\n",
    "            \"\"\"\n",
    "            Calculates composite drought index combining multiple indicators.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Get data for the specified date (monthly composites)\n",
    "            target_date = ee.Date(date)\n",
    "            month_start = target_date.advance(-1, 'month')\n",
    "            \n",
    "            # PDSI (monthly)\n",
    "            pdsi_value = pdsi.filterDate(month_start, target_date).mean()\n",
    "            \n",
    "            # SPI (3-month)\n",
    "            spi_3m = chirps.filterDate(target_date.advance(-3, 'month'), target_date) \\\n",
    "                .sum().subtract(\n",
    "                    chirps.filterDate('1981-01-01', '2010-12-31')\n",
    "                    .filter(ee.Filter.calendarRange(target_date.get('month'), target_date.get('month'), 'month'))\n",
    "                    .sum().reduce(ee.Reducer.mean())\n",
    "                ).divide(\n",
    "                    chirps.filterDate('1981-01-01', '2010-12-31')\n",
    "                    .filter(ee.Filter.calendarRange(target_date.get('month'), target_date.get('month'), 'month'))\n",
    "                    .sum().reduce(ee.Reducer.stdDev())\n",
    "                )\n",
    "            \n",
    "            # Vegetation health anomaly\n",
    "            ndvi_current = modis_ndvi.filterDate(month_start, target_date).mean()\n",
    "            ndvi_climatology = modis_ndvi.filterDate('2001-01-01', '2020-12-31') \\\n",
    "                .filter(ee.Filter.calendarRange(target_date.get('month'), target_date.get('month'), 'month')) \\\n",
    "                .mean()\n",
    "            ndvi_anomaly = ndvi_current.subtract(ndvi_climatology).divide(\n",
    "                modis_ndvi.filterDate('2001-01-01', '2020-12-31') \\\n",
    "                .filter(ee.Filter.calendarRange(target_date.get('month'), target_date.get('month'), 'month')) \\\n",
    "                .reduce(ee.Reducer.stdDev())\n",
    "            )\n",
    "            \n",
    "            # Soil moisture anomaly\n",
    "            sm_current = soil_moisture.filterDate(month_start, target_date).mean()\n",
    "            sm_climatology = soil_moisture.filterDate('1981-01-01', '2010-12-31') \\\n",
    "                .filter(ee.Filter.calendarRange(target_date.get('month'), target_date.get('month'), 'month')) \\\n",
    "                .mean()\n",
    "            sm_anomaly = sm_current.subtract(sm_climatology).divide(\n",
    "                soil_moisture.filterDate('1981-01-01', '2010-12-31') \\\n",
    "                .filter(ee.Filter.calendarRange(target_date.get('month'), target_date.get('month'), 'month')) \\\n",
    "                .reduce(ee.Reducer.stdDev())\n",
    "            )\n",
    "            \n",
    "            # Composite drought index (weighted average)\n",
    "            composite_drought_index = pdsi_value.multiply(0.3) \\\n",
    "                .add(spi_3m.multiply(0.3)) \\\n",
    "                .add(ndvi_anomaly.multiply(0.2)) \\\n",
    "                .add(sm_anomaly.multiply(0.2)) \\\n",
    "                .rename('composite_drought_index')\n",
    "            \n",
    "            # Classify drought severity\n",
    "            drought_severity = ee.Image(0) \\\n",
    "                .where(composite_drought_index.lt(-2), 4) \\\n",
    "                .where(composite_drought_index.gte(-2).And(composite_drought_index.lt(-1.5)), 3) \\\n",
    "                .where(composite_drought_index.gte(-1.5).And(composite_drought_index.lt(-1)), 2) \\\n",
    "                .where(composite_drought_index.gte(-1).And(composite_drought_index.lt(-0.5)), 1) \\\n",
    "                .where(composite_drought_index.gte(-0.5), 0) \\\n",
    "                .rename('drought_severity')\n",
    "            \n",
    "            return ee.Image.cat([composite_drought_index, drought_severity]) \\\n",
    "                .set('system:time_start', target_date.millis())\n",
    "        \n",
    "        return {\n",
    "            'calculator': calculate_drought_composite_index,\n",
    "            'data_sources': {\n",
    "                'pdsi': pdsi,\n",
    "                'chirps': chirps,\n",
    "                'modis_ndvi': modis_ndvi,\n",
    "                'soil_moisture': soil_moisture\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # 2. Flood monitoring system\n",
    "    def setup_flood_monitoring():\n",
    "        \"\"\"\n",
    "        Sets up flood monitoring using SAR and optical data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sentinel-1 SAR for flood detection\n",
    "        s1_collection = ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "        \n",
    "        # Landsat for optical flood detection\n",
    "        landsat_collection = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n",
    "        \n",
    "        def detect_flood_event(event_date, reference_period_days=30):\n",
    "            \"\"\"\n",
    "            Detects flood events by comparing to reference conditions.\n",
    "            \"\"\"\n",
    "            \n",
    "            event_date = ee.Date(event_date)\n",
    "            reference_start = event_date.advance(-reference_period_days-7, 'day')\n",
    "            reference_end = event_date.advance(-7, 'day')\n",
    "            \n",
    "            # SAR-based flood detection\n",
    "            def sar_flood_detection():\n",
    "                # Reference SAR image (median of pre-event period)\n",
    "                reference_sar = s1_collection \\\n",
    "                    .filterBounds(region) \\\n",
    "                    .filterDate(reference_start, reference_end) \\\n",
    "                    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "                    .select('VV') \\\n",
    "                    .median()\n",
    "                \n",
    "                # Event SAR image\n",
    "                event_sar = s1_collection \\\n",
    "                    .filterBounds(region) \\\n",
    "                    .filterDate(event_date, event_date.advance(3, 'day')) \\\n",
    "                    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "                    .select('VV') \\\n",
    "                    .median()\n",
    "                \n",
    "                # Calculate difference (flood areas show decreased backscatter)\n",
    "                sar_difference = reference_sar.subtract(event_sar)\n",
    "                \n",
    "                # Threshold for flood detection (areas with >3dB decrease)\n",
    "                flood_mask_sar = sar_difference.gt(3)\n",
    "                \n",
    "                return flood_mask_sar\n",
    "            \n",
    "            # Optical-based flood detection\n",
    "            def optical_flood_detection():\n",
    "                # Reference optical image\n",
    "                reference_optical = landsat_collection \\\n",
    "                    .filterBounds(region) \\\n",
    "                    .filterDate(reference_start, reference_end) \\\n",
    "                    .map(advanced_cloud_mask) \\\n",
    "                    .median()\n",
    "                \n",
    "                # Event optical image\n",
    "                event_optical = landsat_collection \\\n",
    "                    .filterBounds(region) \\\n",
    "                    .filterDate(event_date, event_date.advance(7, 'day')) \\\n",
    "                    .map(advanced_cloud_mask) \\\n",
    "                    .median()\n",
    "                \n",
    "                # Calculate MNDWI for both periods\n",
    "                def calculate_mndwi(image):\n",
    "                    green = image.select('SR_B3').multiply(0.0000275).add(-0.2)\n",
    "                    swir1 = image.select('SR_B6').multiply(0.0000275).add(-0.2)\n",
    "                    mndwi = green.subtract(swir1).divide(green.add(swir1))\n",
    "                    return image.addBands(mndwi.rename('MNDWI'))\n",
    "                \n",
    "                reference_mndwi = calculate_mndwi(reference_optical).select('MNDWI')\n",
    "                event_mndwi = calculate_mndwi(event_optical).select('MNDWI')\n",
    "                \n",
    "                # Flood detection based on MNDWI increase\n",
    "                mndwi_increase = event_mndwi.subtract(reference_mndwi)\n",
    "                flood_mask_optical = mndwi_increase.gt(0.2).And(event_mndwi.gt(0))\n",
    "                \n",
    "                return flood_mask_optical\n",
    "            \n",
    "            # Combine SAR and optical detections\n",
    "            sar_flood = sar_flood_detection()\n",
    "            optical_flood = optical_flood_detection()\n",
    "            \n",
    "            # Combined flood mask (either SAR or optical detection)\n",
    "            combined_flood_mask = sar_flood.Or(optical_flood)\n",
    "            \n",
    "            # Apply topographic constraints (floods unlikely on steep slopes)\n",
    "            dem = ee.Image('USGS/SRTMGL1_003')\n",
    "            slope = ee.Terrain.slope(dem)\n",
    "            topographic_constraint = slope.lt(10)  # Less than 10 degrees\n",
    "            \n",
    "            final_flood_mask = combined_flood_mask.And(topographic_constraint)\n",
    "            \n",
    "            return {\n",
    "                'flood_mask': final_flood_mask,\n",
    "                'sar_contribution': sar_flood,\n",
    "                'optical_contribution': optical_flood,\n",
    "                'event_date': event_date\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'detector': detect_flood_event,\n",
    "            'data_sources': {\n",
    "                'sentinel1': s1_collection,\n",
    "                'landsat': landsat_collection\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Set up monitoring systems based on requested variables\n",
    "    if 'drought' in monitoring_variables:\n",
    "        monitoring_system['drought'] = setup_drought_monitoring()\n",
    "    \n",
    "    if 'flood' in monitoring_variables:\n",
    "        monitoring_system['flood'] = setup_flood_monitoring()\n",
    "    \n",
    "    # 3. Automated alert system\n",
    "    def create_alert_system(monitoring_results):\n",
    "        \"\"\"\n",
    "        Creates automated alerts based on monitoring results.\n",
    "        \"\"\"\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        # Drought alerts\n",
    "        if 'drought' in monitoring_results:\n",
    "            drought_severity = monitoring_results['drought']['drought_severity']\n",
    "            \n",
    "            # Calculate area under different drought categories\n",
    "            severe_drought_area = drought_severity.gte(3).reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=region,\n",
    "                scale=1000,\n",
    "                maxPixels=1e8\n",
    "            )\n",
    "\n",
    "            \n",
    "            currentdate = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "            alerts.append({\n",
    "                'type': 'drought',\n",
    "                'severity': 'high' if severe_drought_area.get('drought_severity').getInfo() > 100 else 'moderate',\n",
    "                'affected_area_km2': ee.Number(severe_drought_area.get('drought_severity')).multiply(1e-6).getInfo(),\n",
    "                'timestamp': ee.Date(currentdate).format('YYYY-MM-dd HH:mm:ss').getInfo()\n",
    "            })\n",
    "        \n",
    "        # Flood alerts\n",
    "        if 'flood' in monitoring_results:\n",
    "            flood_mask = monitoring_results['flood']['flood_mask']\n",
    "            \n",
    "            flood_area = flood_mask.reduceRegion(\n",
    "                reducer=ee.Reducer.sum(),\n",
    "                geometry=region,\n",
    "                scale=30,\n",
    "                maxPixels=1e8\n",
    "            )\n",
    "            \n",
    "            flood_area_km2 = ee.Number(flood_area.get('flood_mask')).multiply(9e-7).getInfo()\n",
    "            currentdate = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "            if flood_area_km2 > 0.1:  # Alert if flood area > 0.1 km²\n",
    "                alerts.append({\n",
    "                    'type': 'flood',\n",
    "                    'severity': 'high' if flood_area_km2 > 10 else 'moderate',\n",
    "                    'affected_area_km2': flood_area_km2,\n",
    "                    'timestamp': ee.Date(currentdate).format('YYYY-MM-dd HH:mm:ss').getInfo()\n",
    "                })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    monitoring_system['alert_system'] = create_alert_system\n",
    "    \n",
    "    return monitoring_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205fdbee-3d13-4c4b-b774-be096d70e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_operational_monitoring_pipeline(region, output_configuration):\n",
    "    \"\"\"\n",
    "    Creates a complete operational monitoring pipeline for environmental agencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def setup_automated_processing_pipeline():\n",
    "        \"\"\"\n",
    "        Sets up automated processing pipeline with scheduling and data management.\n",
    "        \"\"\"\n",
    "        \n",
    "        pipeline_config = {\n",
    "            'processing_schedule': {\n",
    "                'surface_water': 'weekly',\n",
    "                'vegetation_health': 'bi-weekly', \n",
    "                'climate_variables': 'monthly',\n",
    "                'extreme_events': 'daily'\n",
    "            },\n",
    "            'data_retention': {\n",
    "                'raw_data': '1_year',\n",
    "                'processed_products': '10_years',\n",
    "                'alerts': 'permanent'\n",
    "            },\n",
    "            'quality_control': {\n",
    "                'cloud_threshold': 20,  # Maximum cloud cover percentage\n",
    "                'data_completeness_threshold': 80,  # Minimum data availability\n",
    "                'spatial_completeness_threshold': 90  # Minimum spatial coverage\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        def create_processing_task(analysis_type, schedule):\n",
    "            \"\"\"\n",
    "            Creates processing tasks for different analysis types.\n",
    "            \"\"\"\n",
    "            \n",
    "            if analysis_type == 'surface_water':\n",
    "                def process_surface_water():\n",
    "                    currentdate = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "                    current_date = ee.Date(currentdate)\n",
    "                    analysis_period = (\n",
    "                        current_date.advance(-7, 'day').format('YYYY-MM-dd'),\n",
    "                        current_date.format('YYYY-MM-dd')\n",
    "                    )\n",
    "                    \n",
    "                    water_results = comprehensive_surface_water_mapping(\n",
    "                        region, analysis_period\n",
    "                    )\n",
    "                    \n",
    "                    # Export results\n",
    "                    export_task = ee.batch.Export.image.toDrive(\n",
    "                        image=water_results['water_mask'],\n",
    "                        description=f'surface_water_{current_date.format(\"YYYY_MM_dd\").getInfo()}',\n",
    "                        scale=30,\n",
    "                        region=region,\n",
    "                        maxPixels=1e9\n",
    "                    )\n",
    "                    \n",
    "                    return export_task\n",
    "                \n",
    "                return process_surface_water\n",
    "            \n",
    "            elif analysis_type == 'climate_variables':\n",
    "                def process_climate_data():\n",
    "                    currentdate = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "                    current_date = ee.Date(currentdate)\n",
    "                    analysis_period = (\n",
    "                        current_date.advance(-1, 'month').format('YYYY-MM-dd'),\n",
    "                        current_date.format('YYYY-MM-dd')\n",
    "                    )\n",
    "                    \n",
    "                    climate_results = comprehensive_climate_analysis(\n",
    "                        region, analysis_period\n",
    "                    )\n",
    "                    \n",
    "                    # Export multiple climate variables\n",
    "                    export_tasks = []\n",
    "                    for source_name, source_data in climate_results['climate_data_sources'].items():\n",
    "                        for var_name in source_data['variables'].keys():\n",
    "                            if var_name in ['temperature_max', 'temperature_min', 'precipitation']:\n",
    "                                monthly_composite = source_data['collection'].median()\n",
    "                                \n",
    "                                export_task = ee.batch.Export.image.toDrive(\n",
    "                                    image=monthly_composite.select(source_data['variables'][var_name]),\n",
    "                                    description=f'{var_name}_{source_name}_{current_date.format(\"YYYY_MM\").getInfo()}',\n",
    "                                    scale=5000,\n",
    "                                    region=region,\n",
    "                                    maxPixels=1e9\n",
    "                                )\n",
    "                                \n",
    "                                export_tasks.append(export_task)\n",
    "                    \n",
    "                    return export_tasks\n",
    "                \n",
    "                return process_climate_data\n",
    "            \n",
    "            # Add more processing functions for other analysis types\n",
    "            return None\n",
    "        \n",
    "        # Create processing tasks\n",
    "        processing_tasks = {}\n",
    "        for analysis_type, schedule in pipeline_config['processing_schedule'].items():\n",
    "            processing_tasks[analysis_type] = {\n",
    "                'function': create_processing_task(analysis_type, schedule),\n",
    "                'schedule': schedule,\n",
    "                'last_run': None,\n",
    "                'next_run': None\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'config': pipeline_config,\n",
    "            'tasks': processing_tasks\n",
    "        }\n",
    "    \n",
    "    def setup_data_management_system():\n",
    "        \"\"\"\n",
    "        Sets up data management system with versioning and metadata.\n",
    "        \"\"\"\n",
    "        \n",
    "        data_management = {\n",
    "            'asset_naming_convention': {\n",
    "                'prefix': f\"environmental_monitoring_{region.getInfo()['type']}\",\n",
    "                'date_format': 'YYYY_MM_dd',\n",
    "                'version_format': 'v{major}_{minor}_{patch}'\n",
    "            },\n",
    "            'metadata_schema': {\n",
    "                'required_fields': [\n",
    "                    'processing_date', 'data_source', 'analysis_type',\n",
    "                    'spatial_resolution', 'temporal_coverage', 'quality_score'\n",
    "                ],\n",
    "                'optional_fields': [\n",
    "                    'cloud_coverage', 'data_gaps', 'processing_parameters',\n",
    "                    'accuracy_assessment', 'validation_results'\n",
    "                ]\n",
    "            },\n",
    "            'quality_assurance': {\n",
    "                'automated_checks': [\n",
    "                    'spatial_completeness', 'temporal_consistency',\n",
    "                    'value_ranges', 'metadata_completeness'\n",
    "                ],\n",
    "                'manual_validation': [\n",
    "                    'accuracy_assessment', 'anomaly_investigation',\n",
    "                    'user_feedback_integration'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        def generate_product_metadata(image, analysis_type, processing_params):\n",
    "            \"\"\"\n",
    "            Generates comprehensive metadata for processed products.\n",
    "            \"\"\"\n",
    "            currentdate = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "            metadata = {\n",
    "                'product_id': ee.String(data_management['asset_naming_convention']['prefix']) \\\n",
    "                    .cat('_').cat(ee.Date(currentdate).format('YYYY_MM_dd')) \\\n",
    "                    .cat('_').cat(analysis_type),\n",
    "                'processing_date': ee.Date(currentdate).format('YYYY-MM-dd HH:mm:ss'),\n",
    "                'analysis_type': analysis_type,\n",
    "                'spatial_bounds': region.bounds().getInfo(),\n",
    "                'processing_parameters': processing_params,\n",
    "                'data_quality_score': image.get('quality_score', -1)\n",
    "            }\n",
    "            \n",
    "            return metadata\n",
    "        \n",
    "        return {\n",
    "            'config': data_management,\n",
    "            'metadata_generator': generate_product_metadata\n",
    "        }\n",
    "    \n",
    "    def setup_monitoring_dashboard():\n",
    "        \"\"\"\n",
    "        Sets up monitoring dashboard configuration for real-time status tracking.\n",
    "        \"\"\"\n",
    "        \n",
    "        dashboard_config = {\n",
    "            'update_frequency': 'hourly',\n",
    "            'key_indicators': [\n",
    "                'current_water_extent',\n",
    "                'drought_severity_level',\n",
    "                'extreme_event_alerts',\n",
    "                'data_processing_status',\n",
    "                'system_health_metrics'\n",
    "            ],\n",
    "            'visualization_types': {\n",
    "                'time_series_charts': ['precipitation', 'temperature', 'water_extent'],\n",
    "                'maps': ['current_conditions', 'anomalies', 'alerts'],\n",
    "                'statistics': ['summary_stats', 'trend_indicators', 'quality_metrics']\n",
    "            },\n",
    "            'alert_thresholds': {\n",
    "                'drought_severity': {'moderate': 2, 'severe': 3, 'extreme': 4},\n",
    "                'flood_area_km2': {'moderate': 1, 'severe': 10, 'extreme': 50},\n",
    "                'data_latency_hours': {'warning': 6, 'critical': 24},\n",
    "                'processing_failure_rate': {'warning': 0.05, 'critical': 0.1}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        def generate_dashboard_data():\n",
    "            \"\"\"\n",
    "            Generates data for the monitoring dashboard.\n",
    "            \"\"\"\n",
    "            currentdate = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "            current_date = ee.Date(currentdate)\n",
    "            \n",
    "            # Current status indicators\n",
    "            status_indicators = {\n",
    "                'last_update': current_date.format('YYYY-MM-dd HH:mm:ss'),\n",
    "                'system_status': 'operational',  # operational, maintenance, error\n",
    "                'active_alerts': [],\n",
    "                'data_freshness': {}\n",
    "            }\n",
    "            \n",
    "            # Generate summary statistics\n",
    "            def calculate_summary_statistics():\n",
    "                \"\"\"Calculate key summary statistics for dashboard.\"\"\"\n",
    "                \n",
    "                # Recent water extent\n",
    "                recent_water = comprehensive_surface_water_mapping(\n",
    "                    region, \n",
    "                    (current_date.advance(-7, 'day').format('YYYY-MM-dd'), \n",
    "                     current_date.format('YYYY-MM-dd'))\n",
    "                )\n",
    "                \n",
    "                water_extent_km2 = recent_water['water_mask'].reduceRegion(\n",
    "                    reducer=ee.Reducer.sum(),\n",
    "                    geometry=region,\n",
    "                    scale=30,\n",
    "                    maxPixels=1e9\n",
    "                ).get('water_mask')\n",
    "                \n",
    "                # Recent climate conditions\n",
    "                recent_climate = comprehensive_climate_analysis(\n",
    "                    region,\n",
    "                    (current_date.advance(-30, 'day').format('YYYY-MM-dd'),\n",
    "                     current_date.format('YYYY-MM-dd'))\n",
    "                )\n",
    "                \n",
    "                summary_stats = {\n",
    "                    'water_extent_km2': ee.Number(water_extent_km2).multiply(9e-7).getInfo(),\n",
    "                    'region_area_km2': region.area().divide(1e6).getInfo(),\n",
    "                    'water_percentage': ee.Number(water_extent_km2).multiply(9e-7).divide(region.area().divide(1e6)).multiply(100).getInfo()\n",
    "                }\n",
    "                \n",
    "                return summary_stats\n",
    "            \n",
    "            summary_stats = calculate_summary_statistics()\n",
    "            status_indicators['summary_statistics'] = summary_stats\n",
    "            \n",
    "            return status_indicators\n",
    "        \n",
    "        return {\n",
    "            'config': dashboard_config,\n",
    "            'data_generator': generate_dashboard_data\n",
    "        }\n",
    "    \n",
    "    # Integration with external systems\n",
    "    def setup_external_integrations():\n",
    "        \"\"\"\n",
    "        Sets up integrations with external systems and APIs.\n",
    "        \"\"\"\n",
    "        \n",
    "        integrations = {\n",
    "            'weather_services': {\n",
    "                'national_weather_service': {\n",
    "                    'api_endpoint': 'https://api.weather.gov/',\n",
    "                    'data_types': ['forecasts', 'warnings', 'observations'],\n",
    "                    'update_frequency': 'hourly'\n",
    "                },\n",
    "                'global_forecast_system': {\n",
    "                    'data_source': 'NOAA/GFS0P25',\n",
    "                    'variables': ['temperature_2m', 'precipitation_surface', 'relative_humidity_2m'],\n",
    "                    'forecast_horizon': '7_days'\n",
    "                }\n",
    "            },\n",
    "            'hydrological_services': {\n",
    "                'usgs_streamflow': {\n",
    "                    'api_endpoint': 'https://waterservices.usgs.gov/nwis/iv/',\n",
    "                    'parameters': ['discharge', 'gage_height', 'temperature'],\n",
    "                    'stations': []  # Would be populated with relevant station IDs\n",
    "                },\n",
    "                'reservoir_levels': {\n",
    "                    'data_source': 'reservoir_monitoring_network',\n",
    "                    'update_frequency': 'daily'\n",
    "                }\n",
    "            },\n",
    "            'emergency_management': {\n",
    "                'alert_distribution': {\n",
    "                    'email_notifications': True,\n",
    "                    'sms_alerts': True,\n",
    "                    'web_services': ['fema_integration', 'state_emergency_management'],\n",
    "                    'social_media': ['twitter_api', 'emergency_broadcast']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        def create_data_fusion_workflow(gee_results, external_data):\n",
    "            \"\"\"\n",
    "            Creates workflow to fuse GEE results with external data sources.\n",
    "            \"\"\"\n",
    "            \n",
    "            fused_analysis = {}\n",
    "            \n",
    "            # Combine GEE water extent with USGS streamflow data\n",
    "            if 'surface_water' in gee_results and 'usgs_streamflow' in external_data:\n",
    "                # This would involve API calls to USGS and correlation analysis\n",
    "                # Simplified representation here\n",
    "                fused_analysis['water_flow_correlation'] = {\n",
    "                    'gee_water_extent': gee_results['surface_water']['water_extent_km2'],\n",
    "                    'streamflow_data': external_data['usgs_streamflow'],\n",
    "                    'correlation_analysis': 'to_be_implemented'\n",
    "                }\n",
    "            \n",
    "            # Combine GEE drought indicators with agricultural reports\n",
    "            if 'drought_monitoring' in gee_results:\n",
    "                fused_analysis['agricultural_impact'] = {\n",
    "                    'drought_severity': gee_results['drought_monitoring']['composite_drought_index'],\n",
    "                    'crop_condition_reports': external_data.get('agricultural_reports', {}),\n",
    "                    'economic_impact_estimate': 'to_be_calculated'\n",
    "                }\n",
    "            \n",
    "            return fused_analysis\n",
    "        \n",
    "        return {\n",
    "            'config': integrations,\n",
    "            'fusion_workflow': create_data_fusion_workflow\n",
    "        }\n",
    "    \n",
    "    # Assemble complete operational pipeline\n",
    "    currentdate = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "    operational_pipeline = {\n",
    "        'processing_pipeline': setup_automated_processing_pipeline(),\n",
    "        'data_management': setup_data_management_system(),\n",
    "        'monitoring_dashboard': setup_monitoring_dashboard(),\n",
    "        'external_integrations': setup_external_integrations(),\n",
    "        'configuration': {\n",
    "            'region': region,\n",
    "            'output_config': output_configuration,\n",
    "            'deployment_date': ee.Date(currentdate).format('YYYY-MM-dd').getInfo(),\n",
    "            'version': '1.0.0'\n",
    "        }\n",
    "    }\n",
    "    return operational_pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955abb04-7773-4475-ba9f-3c7db6edf451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region of interest defined: {'type': 'Polygon', 'coordinates': [[[-120, 39], [-119.9, 39], [-119.9, 39.1], [-120, 39.1], [-120, 39]]]}\n",
      "\n",
      "--- Running Comprehensive Climate Analysis ---\n",
      "Climate Analysis Results (partial view):\n",
      "dict_keys(['climate_data_sources', 'trend_analysis', 'extreme_analysis', 'impact_assessment'])\n",
      "\n",
      "--- Running Operational Monitoring Pipeline Example ---\n",
      "Operational Pipeline Setup (partial view):\n",
      "dict_keys(['surface_water', 'vegetation_health', 'climate_variables', 'extreme_events'])\n",
      "\n",
      "--- Running Extreme Event Monitoring System Example ---\n",
      "Extreme Event Monitoring System Setup (partial view):\n",
      "dict_keys(['drought', 'flood', 'alert_system'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colla\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ee\\deprecation.py:209: DeprecationWarning: \n",
      "\n",
      "Attention required for MODIS/006/MOD13A1! You are using a deprecated asset.\n",
      "To make sure your code keeps working, please update it.\n",
      "Learn more: https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD13A1\n",
      "\n",
      "  warnings.warn(warning, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Example: Lake Tahoe region (polygon coordinates)\n",
    "region = ee.Geometry.Polygon(\n",
    "[[[-120.00, 39.00],\n",
    "[-119.90, 39.00],\n",
    "[-119.90, 39.10],\n",
    "[-120.00, 39.10],\n",
    "[-120.00, 39.00]]])\n",
    "\n",
    "print(f\"Region of interest defined: {region.getInfo()}\")\n",
    "\n",
    "# --- Running Examples ---\n",
    "\n",
    "# Example 0: Comprehensive Climate Analysis\n",
    "print(\"\\n--- Running Comprehensive Climate Analysis ---\")\n",
    "analysis_period_climate = ('2020-01-01', '2021-01-01')\n",
    "climate_results = comprehensive_climate_analysis(region, analysis_period_climate)\n",
    "print(\"Climate Analysis Results (partial view):\")\n",
    "print(climate_results.keys())\n",
    "\n",
    "# Example 1: Create Operational Monitoring Pipeline\n",
    "print(\"\\n--- Running Operational Monitoring Pipeline Example ---\")\n",
    "output_config_pipeline = {\"export_folder\": \"GEE_Monitoring_Outputs\"}\n",
    "operational_pipeline = create_operational_monitoring_pipeline(region, output_config_pipeline)\n",
    "print(\"Operational Pipeline Setup (partial view):\")\n",
    "print(operational_pipeline[\"processing_pipeline\"][\"tasks\"].keys())\n",
    "\n",
    "# Example 2: Create Extreme Event Monitoring System\n",
    "print(\"\\n--- Running Extreme Event Monitoring System Example ---\")\n",
    "monitoring_vars = [\"drought\", \"flood\"] # Limiting for a quicker example\n",
    "extreme_event_system = create_extreme_event_monitoring_system(region, monitoring_vars)\n",
    "print(\"Extreme Event Monitoring System Setup (partial view):\")\n",
    "print(extreme_event_system.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08a9b9e8-8fc3-4f7c-8b6a-851b28b5b3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drought': {'calculator': <function __main__.create_extreme_event_monitoring_system.<locals>.setup_drought_monitoring.<locals>.calculate_drought_composite_index(date)>,\n",
       "  'data_sources': {'pdsi': <ee.imagecollection.ImageCollection at 0x23229756630>,\n",
       "   'chirps': <ee.imagecollection.ImageCollection at 0x23229756810>,\n",
       "   'modis_ndvi': <ee.imagecollection.ImageCollection at 0x23229757410>,\n",
       "   'soil_moisture': <ee.imagecollection.ImageCollection at 0x232297575f0>}},\n",
       " 'flood': {'detector': <function __main__.create_extreme_event_monitoring_system.<locals>.setup_flood_monitoring.<locals>.detect_flood_event(event_date, reference_period_days=30)>,\n",
       "  'data_sources': {'sentinel1': <ee.imagecollection.ImageCollection at 0x23229757710>,\n",
       "   'landsat': <ee.imagecollection.ImageCollection at 0x23229757650>}},\n",
       " 'alert_system': <function __main__.create_extreme_event_monitoring_system.<locals>.create_alert_system(monitoring_results)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extreme_event_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "187767bf-1f60-44cd-b6a8-ec493debd82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def ic_to_dataframe(ic_dict, source, variable, region, scale=10000):\n",
    "    \"\"\"\n",
    "    Convert an ImageCollection in your dictionary to a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        ic_dict (dict): The full dictionary with ImageCollection references.\n",
    "        source (str): e.g., 'era5_monthly', 'terraclimate', 'gridmet'.\n",
    "        variable (str): e.g., 'temperature_2m', 'precipitation'.\n",
    "        region (ee.Geometry): Earth Engine geometry to sample.\n",
    "        scale (int): Spatial resolution in meters.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: columns=['date', 'value'].\n",
    "    \"\"\"\n",
    "    # Get the ImageCollection and the corresponding band\n",
    "    collection = ic_dict['climate_data_sources'][source]['collection']\n",
    "    band_name = ic_dict['climate_data_sources'][source]['variables'][variable]\n",
    "\n",
    "    # Convert collection to a list\n",
    "    ic_list = collection.toList(collection.size())\n",
    "    data = []\n",
    "\n",
    "    for i in range(collection.size().getInfo()):\n",
    "        img = ee.Image(ic_list.get(i))\n",
    "        # print(img.getInfo())\n",
    "        date = img.date().format('YYYY-MM-dd').getInfo()\n",
    "        value = img.reduceRegion(\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            geometry=region,\n",
    "            scale=scale\n",
    "        ).get(band_name).getInfo()\n",
    "        data.append({'date': date, 'value': value})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = ic_to_dataframe(climate_results, \"era5_monthly\", \"temperature_2m\", region, scale=500)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f5a598-1eff-42fe-ac03-40547e9adca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>273.884583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>274.092499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>273.984650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>278.569580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>283.736420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>287.967224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       value\n",
       "0  2020-01-01  273.884583\n",
       "1  2020-02-01  274.092499\n",
       "2  2020-03-01  273.984650\n",
       "3  2020-04-01  278.569580\n",
       "4  2020-05-01  283.736420\n",
       "5  2020-06-01  287.967224"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ic_to_dataframe(climate_results, \"era5_monthly\", \"temperature_2m\", region, scale=500)\n",
    "df.head(10)\n",
    "#note temperature are in K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90bf581d-9c4f-4285-bd8c-e45e97466d48",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1908881956.py, line 42)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m.set('interpolated', True)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def implement_advanced_solutions():\n",
    "    \"\"\"\n",
    "    Implements solutions for common challenges in operational Earth observation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Data gap handling and interpolation\n",
    "    def handle_data_gaps(image_collection, max_gap_days=30):\n",
    "        \"\"\"\n",
    "        Handles data gaps using spatiotemporal interpolation methods.\n",
    "        \"\"\"\n",
    "        \n",
    "        def interpolate_temporal_gaps(collection):\n",
    "            \"\"\"Interpolates missing values in time series.\"\"\"\n",
    "            \n",
    "            # Convert collection to list for gap detection\n",
    "            image_list = collection.toList(collection.size())\n",
    "            dates_list = collection.aggregate_array('system:time_start')\n",
    "            \n",
    "            def fill_gap(current_index):\n",
    "                \"\"\"Fill gap at current index using neighboring images.\"\"\"\n",
    "                \n",
    "                current_image = ee.Image(image_list.get(current_index))\n",
    "                current_date = ee.Date(current_image.get('system:time_start'))\n",
    "                \n",
    "                # Find previous and next valid images\n",
    "                prev_image = ee.Image(image_list.get(ee.Number(current_index).subtract(1).max(0)))\n",
    "                next_image = ee.Image(image_list.get(ee.Number(current_index).add(1).min(collection.size().subtract(1))))\n",
    "                \n",
    "                # Temporal interpolation\n",
    "                prev_date = ee.Date(prev_image.get('system:time_start'))\n",
    "                next_date = ee.Date(next_image.get('system:time_start'))\n",
    "                \n",
    "                # Calculate interpolation weights\n",
    "                total_gap = next_date.difference(prev_date, 'day')\n",
    "                weight_next = current_date.difference(prev_date, 'day').divide(total_gap)\n",
    "                weight_prev = ee.Number(1).subtract(weight_next)\n",
    "                \n",
    "                # Weighted interpolation\n",
    "                interpolated = prev_image.multiply(weight_prev).add(next_image.multiply(weight_next))\n",
    "                \n",
    "                return interpolated.set('system:time_start', current_date.millis())\n",
    "                    .set('interpolated', True)\n",
    "            \n",
    "            # Apply interpolation to identified gaps\n",
    "            # This is a simplified version - production systems would use more sophisticated gap detection\n",
    "            return collection\n",
    "        \n",
    "        def spatial_interpolation(image, reference_images):\n",
    "            \"\"\"Fills spatial gaps using neighboring pixels and reference data.\"\"\"\n",
    "            \n",
    "            # Identify missing pixels\n",
    "            valid_pixels = image.mask()\n",
    "            \n",
    "            # Use focal mean for spatial interpolation\n",
    "            spatial_fill = image.focalMean(radius=3, kernelType='circle', units='pixels')\n",
    "            \n",
    "            # Combine original and interpolated values\n",
    "            filled_image = image.where(valid_pixels.eq(0), spatial_fill)\n",
    "            \n",
    "            return filled_image.set('spatial_interpolation_applied', True)\n",
    "        \n",
    "        return {\n",
    "            'temporal_interpolation': interpolate_temporal_gaps,\n",
    "            'spatial_interpolation': spatial_interpolation\n",
    "        }\n",
    "    \n",
    "    # 2. Multi-sensor calibration and harmonization\n",
    "    def implement_sensor_harmonization():\n",
    "        \"\"\"\n",
    "        Implements cross-sensor calibration and harmonization methods.\n",
    "        \"\"\"\n",
    "        \n",
    "        def harmonize_landsat_sentinel(landsat_image, sentinel_image):\n",
    "            \"\"\"\n",
    "            Harmonizes Landsat and Sentinel-2 reflectance values.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Landsat 8 to Sentinel-2 harmonization coefficients\n",
    "            # Based on published cross-calibration studies\n",
    "            harmonization_coefficients = {\n",
    "                'blue': {'slope': 0.9959, 'intercept': -0.0002},\n",
    "                'green': {'slope': 0.9778, 'intercept': -0.004},\n",
    "                'red': {'slope': 0.9837, 'intercept': -0.0023},\n",
    "                'nir': {'slope': 0.9683, 'intercept': 0.0084}\n",
    "            }\n",
    "            \n",
    "            def apply_harmonization(image, sensor_type):\n",
    "                \"\"\"Apply harmonization coefficients to image.\"\"\"\n",
    "                \n",
    "                if sensor_type == 'landsat_to_sentinel':\n",
    "                    # Convert Landsat to Sentinel-2 equivalent\n",
    "                    blue_harm = image.select('SR_B2').multiply(0.0000275).add(-0.2) \\\n",
    "                        .multiply(harmonization_coefficients['blue']['slope']) \\\n",
    "                        .add(harmonization_coefficients['blue']['intercept'])\n",
    "                    \n",
    "                    green_harm = image.select('SR_B3').multiply(0.0000275).add(-0.2) \\\n",
    "                        .multiply(harmonization_coefficients['green']['slope']) \\\n",
    "                        .add(harmonization_coefficients['green']['intercept'])\n",
    "                    \n",
    "                    red_harm = image.select('SR_B4').multiply(0.0000275).add(-0.2) \\\n",
    "                        .multiply(harmonization_coefficients['red']['slope']) \\\n",
    "                        .add(harmonization_coefficients['red']['intercept'])\n",
    "                    \n",
    "                    nir_harm = image.select('SR_B5').multiply(0.0000275).add(-0.2) \\\n",
    "                        .multiply(harmonization_coefficients['nir']['slope']) \\\n",
    "                        .add(harmonization_coefficients['nir']['intercept'])\n",
    "                    \n",
    "                    return ee.Image.cat([blue_harm, green_harm, red_harm, nir_harm]) \\\n",
    "                        .rename(['blue', 'green', 'red', 'nir']) \\\n",
    "                        .set('harmonized', True, 'source_sensor', 'landsat_harmonized_to_sentinel')\n",
    "                \n",
    "                return image\n",
    "            \n",
    "            harmonized_landsat = apply_harmonization(landsat_image, 'landsat_to_sentinel')\n",
    "            \n",
    "            return {\n",
    "                'harmonized_landsat': harmonized_landsat,\n",
    "                'original_sentinel': sentinel_image,\n",
    "                'harmonization_applied': True\n",
    "            }\n",
    "        \n",
    "        return harmonize_landsat_sentinel\n",
    "    \n",
    "    # 3. Uncertainty quantification and propagation\n",
    "    def implement_uncertainty_analysis():\n",
    "        \"\"\"\n",
    "        Implements comprehensive uncertainty analysis for Earth observation products.\n",
    "        \"\"\"\n",
    "        \n",
    "        def calculate_measurement_uncertainty(image, sensor_specs):\n",
    "            \"\"\"\n",
    "            Calculates measurement uncertainty based on sensor specifications.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Sensor-specific uncertainty parameters\n",
    "            uncertainty_params = {\n",
    "                'landsat8': {\n",
    "                    'radiometric_uncertainty': 3,  # %\n",
    "                    'geometric_uncertainty': 12,   # meters\n",
    "                    'spectral_uncertainty': 1      # %\n",
    "                },\n",
    "                'sentinel2': {\n",
    "                    'radiometric_uncertainty': 5,  # %\n",
    "                    'geometric_uncertainty': 20,   # meters  \n",
    "                    'spectral_uncertainty': 2      # %\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            sensor_type = sensor_specs.get('sensor_type', 'landsat8')\n",
    "            params = uncertainty_params[sensor_type]\n",
    "            \n",
    "            # Calculate pixel-wise uncertainty\n",
    "            radiometric_uncertainty = image.multiply(params['radiometric_uncertainty'] / 100.0)\n",
    "            \n",
    "            # Geometric uncertainty affects spatial products\n",
    "            geometric_uncertainty_map = ee.Image.constant(params['geometric_uncertainty'])\n",
    "            \n",
    "            return {\n",
    "                'radiometric_uncertainty': radiometric_uncertainty,\n",
    "                'geometric_uncertainty': geometric_uncertainty_map,\n",
    "                'combined_uncertainty': radiometric_uncertainty.hypot(\n",
    "                    ee.Image.constant(params['spectral_uncertainty'] / 100.0)\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        def propagate_uncertainty_through_analysis(input_uncertainties, analysis_function):\n",
    "            \"\"\"\n",
    "            Propagates uncertainty through analytical workflows using Monte Carlo methods.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Simplified Monte Carlo uncertainty propagation\n",
    "            n_iterations = 100\n",
    "            \n",
    "            def monte_carlo_iteration(iteration):\n",
    "                \"\"\"Single Monte Carlo iteration with perturbed inputs.\"\"\"\n",
    "                \n",
    "                # Add random noise based on uncertainty estimates\n",
    "                noise_factor = ee.Number.random().subtract(0.5).multiply(2)  # -1 to 1\n",
    "                perturbed_input = input_uncertainties['input_image'].add(\n",
    "                    input_uncertainties['uncertainty_image'].multiply(noise_factor)\n",
    "                )\n",
    "                \n",
    "                # Apply analysis function to perturbed input\n",
    "                result = analysis_function(perturbed_input)\n",
    "                \n",
    "                return result.set('iteration', iteration)\n",
    "            \n",
    "            # Run Monte Carlo iterations\n",
    "            iterations = ee.List.sequence(0, n_iterations-1)\n",
    "            monte_carlo_results = iterations.map(monte_carlo_iteration)\n",
    "            \n",
    "            # Calculate uncertainty statistics\n",
    "            results_collection = ee.ImageCollection.fromImages(monte_carlo_results)\n",
    "            mean_result = results_collection.mean()\n",
    "            std_result = results_collection.reduce(ee.Reducer.stdDev())\n",
    "            \n",
    "            return {\n",
    "                'mean_result': mean_result,\n",
    "                'uncertainty_estimate': std_result,\n",
    "                'confidence_interval': {\n",
    "                    'lower_95': results_collection.reduce(ee.Reducer.percentile([2.5])),\n",
    "                    'upper_95': results_collection.reduce(ee.Reducer.percentile([97.5]))\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'measurement_uncertainty': calculate_measurement_uncertainty,\n",
    "            'uncertainty_propagation': propagate_uncertainty_through_analysis\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'data_gap_handling': handle_data_gaps,\n",
    "        'sensor_harmonization': implement_sensor_harmonization(),\n",
    "        'uncertainty_analysis': implement_uncertainty_analysis()\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
